{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5e6a455-6ec8-43e4-a7e1-25c1be05817d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Get authentication token\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "# Working endpoint URL\n",
    "LLAMA_ENDPOINT = \"https://dbc-4b63042e-6da3.cloud.databricks.com/serving-endpoints/databricks-meta-llama-3-3-70b-instruct/invocations\"\n",
    "\n",
    "print(f\"✅ Token obtained\")\n",
    "print(f\"✅ Endpoint: {LLAMA_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030aaebd-b510-44c9-a135-0183b1b7ab51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DBTITLE 1,Test the Endpoint\n",
    "# Quick test to verify access\n",
    "test_payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Say 'Hello, I am working!'\"}\n",
    "    ],\n",
    "    \"max_tokens\": 20,\n",
    "    \"temperature\": 0.1\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(LLAMA_ENDPOINT, json=test_payload, headers=headers, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    print(\"✅ Endpoint test successful!\")\n",
    "    print(f\"Response: {result['choices'][0]['message']['content']}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Endpoint test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82d304af-bc67-479e-85cb-8735efe56c79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def extract_entities_llama(text):\n",
    "    \"\"\"\n",
    "    Extract clinical entities using Llama 3.3 70B\n",
    "    \n",
    "    Returns JSON with: chief_complaint, symptoms, diagnoses, medications, procedures, vital_signs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle empty or very short text\n",
    "    if not text or len(text) < 50:\n",
    "        return json.dumps({\n",
    "            \"chief_complaint\": \"\",\n",
    "            \"symptoms\": [],\n",
    "            \"diagnoses\": [],\n",
    "            \"medications\": [],\n",
    "            \"procedures\": [],\n",
    "            \"vital_signs\": {},\n",
    "            \"extraction_confidence\": \"low\"\n",
    "        })\n",
    "    \n",
    "    # Truncate very long text (Llama context limit)\n",
    "    text_truncated = text[:4000] if len(text) > 4000 else text\n",
    "    \n",
    "    # Craft extraction prompt\n",
    "    prompt = f\"\"\"You are a medical AI assistant. Extract structured clinical information from this medical note.\n",
    "\n",
    "MEDICAL NOTE:\n",
    "{text_truncated}\n",
    "\n",
    "Extract and return ONLY valid JSON in this exact format:\n",
    "{{\n",
    "  \"chief_complaint\": \"primary reason for visit\",\n",
    "  \"symptoms\": [\n",
    "    {{\"name\": \"symptom name\", \"severity\": \"mild/moderate/severe\", \"duration\": \"how long\"}}\n",
    "  ],\n",
    "  \"diagnoses\": [\n",
    "    {{\"diagnosis\": \"diagnosis name\", \"icd10\": \"code if mentioned\", \"status\": \"confirmed/suspected\"}}\n",
    "  ],\n",
    "  \"medications\": [\n",
    "    {{\"name\": \"medication name\", \"dosage\": \"dosage\", \"frequency\": \"frequency\", \"route\": \"route\"}}\n",
    "  ],\n",
    "  \"procedures\": [\n",
    "    {{\"procedure\": \"procedure name\", \"cpt\": \"code if mentioned\"}}\n",
    "  ],\n",
    "  \"vital_signs\": {{\n",
    "    \"blood_pressure\": \"value\",\n",
    "    \"temperature\": \"value with unit\",\n",
    "    \"heart_rate\": \"value\",\n",
    "    \"respiratory_rate\": \"value\",\n",
    "    \"oxygen_saturation\": \"value\"\n",
    "  }},\n",
    "  \"allergies\": [\"allergy1\", \"allergy2\"],\n",
    "  \"extraction_confidence\": \"high/medium/low\"\n",
    "}}\n",
    "\n",
    "IMPORTANT:\n",
    "- Extract ONLY information explicitly stated in the text\n",
    "- Do NOT infer or assume information\n",
    "- Use null for missing fields\n",
    "- Return ONLY the JSON object, no explanations\n",
    "- If a section is empty, use empty arrays [] or empty objects {{}}\n",
    "\n",
    "JSON OUTPUT:\"\"\"\n",
    "\n",
    "    # API payload\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 2000,\n",
    "        \"temperature\": 0.1  # Low temperature for factual extraction\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Call Llama API\n",
    "        response = requests.post(LLAMA_ENDPOINT, json=payload, headers=headers, timeout=90)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        \n",
    "        # Extract content from response\n",
    "        content = result.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"{}\")\n",
    "        \n",
    "        # Clean up markdown code blocks if present\n",
    "        if content.startswith(\"```\"):\n",
    "            # Remove markdown formatting\n",
    "            lines = content.split(\"\\n\")\n",
    "            content = \"\\n\".join([l for l in lines if not l.startswith(\"```\")])\n",
    "            # Remove 'json' tag if present\n",
    "            if content.strip().startswith(\"json\"):\n",
    "                content = content.strip()[4:]\n",
    "        \n",
    "        # Validate it's proper JSON\n",
    "        parsed = json.loads(content.strip())\n",
    "        \n",
    "        # Return cleaned JSON string\n",
    "        return json.dumps(parsed)\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        return json.dumps({\"error\": \"API timeout\", \"extraction_confidence\": \"failed\"})\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return json.dumps({\"error\": f\"API error: {str(e)}\", \"extraction_confidence\": \"failed\"})\n",
    "    except json.JSONDecodeError as e:\n",
    "        return json.dumps({\"error\": f\"JSON parse error: {str(e)}\", \"raw_response\": content[:200], \"extraction_confidence\": \"failed\"})\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Unexpected error: {str(e)}\", \"extraction_confidence\": \"failed\"})\n",
    "\n",
    "print(\"✅ Extraction UDF defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40697788-839a-4c47-a602-06d27f473674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read from bronze/raw zone\n",
    "parsed_df = spark.table(\"healthcare_catalog.raw_zone.parsed_documents\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"Total documents: {parsed_df.count()}\")\n",
    "display(parsed_df.select(\"path\", \"text\").limit(3))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Extract Entities from All Documents\n",
    "# Apply extraction UDF\n",
    "print(\"Starting entity extraction...\")\n",
    "\n",
    "extracted = parsed_df.withColumn(\"entities\", extract_entities_llama(col(\"text\")))\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\n✅ Extraction complete! Sample results:\")\n",
    "display(extracted.select(\"path\", \"entities\").limit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02742026-ab48-4455-8bd8-3dfe56ed872c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DBTITLE 1,Validate Extraction Quality\n",
    "# Check for extraction errors\n",
    "from pyspark.sql.functions import get_json_object\n",
    "\n",
    "extraction_stats = extracted.select(\n",
    "    (get_json_object(col(\"entities\"), \"$.error\").isNotNull()).alias(\"has_error\"),\n",
    "    get_json_object(col(\"entities\"), \"$.extraction_confidence\").alias(\"confidence\")\n",
    ").groupBy(\"has_error\", \"confidence\").count()\n",
    "\n",
    "print(\"Extraction Statistics:\")\n",
    "display(extraction_stats)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Filter Out Failed Extractions\n",
    "# Keep only successful extractions\n",
    "successful_extractions = extracted.filter(\n",
    "    get_json_object(col(\"entities\"), \"$.error\").isNull()\n",
    ")\n",
    "\n",
    "print(f\"✅ Successful extractions: {successful_extractions.count()}\")\n",
    "print(f\"❌ Failed extractions: {extracted.count() - successful_extractions.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e321e4f5-dcd4-49b7-b7fd-4d63e5e1498f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DBTITLE 1,Save to Silver Zone\n",
    "# Save extracted entities to Delta table\n",
    "successful_extractions.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"healthcare_catalog.silver_zone.extracted_entities\")\n",
    "\n",
    "print(\"✅ Saved to: healthcare_catalog.silver_zone.extracted_entities\")\n",
    "\n",
    "# Verify the save\n",
    "result_count = spark.table(\"healthcare_catalog.silver_zone.extracted_entities\").count()\n",
    "print(f\"✅ Verified: {result_count} records in silver zone\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,View Sample Extracted Entities\n",
    "# Display nicely formatted results\n",
    "sample_entities = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        path,\n",
    "        get_json_object(entities, '$.chief_complaint') as chief_complaint,\n",
    "        get_json_object(entities, '$.symptoms') as symptoms,\n",
    "        get_json_object(entities, '$.diagnoses') as diagnoses,\n",
    "        get_json_object(entities, '$.medications') as medications,\n",
    "        get_json_object(entities, '$.extraction_confidence') as confidence\n",
    "    FROM healthcare_catalog.silver_zone.extracted_entities\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(sample_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa0dc81c-bf2a-4e8f-acb2-46ca3277e5cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show final stats\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_documents,\n",
    "        SUM(CASE WHEN get_json_object(entities, '$.extraction_confidence') = 'high' THEN 1 ELSE 0 END) as high_confidence,\n",
    "        SUM(CASE WHEN get_json_object(entities, '$.extraction_confidence') = 'medium' THEN 1 ELSE 0 END) as medium_confidence,\n",
    "        SUM(CASE WHEN get_json_object(entities, '$.extraction_confidence') = 'low' THEN 1 ELSE 0 END) as low_confidence\n",
    "    FROM healthcare_catalog.silver_zone.extracted_entities\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "llama_entity_extraction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
