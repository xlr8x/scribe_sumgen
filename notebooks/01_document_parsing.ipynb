{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51bbaac7-018c-4b84-8caf-7cde7ba57337",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install PyMuPDF\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fcf080c-b42c-4659-865b-dfa62380b89c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "PROJECT_ROOT = \"/Workspace/scribe_sumgen\"\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(f\"âœ… Python path configured: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24e53de8-e5f6-4753-a91a-9cc3b0178fd3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import mlflow\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('src'))  # Ensure src is in the Python path\n",
    "\n",
    "# Schema for parsed documents\n",
    "parsed_schema = StructType([\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"sections\", StringType(), True),  # JSON string\n",
    "    StructField(\"tables\", StringType(), True),     # JSON string\n",
    "    StructField(\"metadata\", StringType(), True),   # JSON string\n",
    "    StructField(\"parse_quality_score\", DoubleType(), True),\n",
    "    StructField(\"parse_errors\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Register parsing UDF\n",
    "from src.parsers.pdf_parser import ClinicalPDFParser \n",
    "\n",
    "@udf(returnType=parsed_schema)\n",
    "def parse_clinical_pdf(pdf_bytes):\n",
    "    \"\"\"Parse PDF bytes and return structured output\"\"\"\n",
    "    import tempfile\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    parser = ClinicalPDFParser()\n",
    "\n",
    "    try:\n",
    "        # Write bytes to temp file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:\n",
    "            tmp.write(pdf_bytes)\n",
    "            tmp_path = tmp.name\n",
    "\n",
    "        # Parse PDF\n",
    "        result = parser.parse_pdf(tmp_path)\n",
    "\n",
    "        # Calculate quality score\n",
    "        quality_score = calculate_parse_quality(result)\n",
    "\n",
    "        # Clean up\n",
    "        os.unlink(tmp_path)\n",
    "\n",
    "        return {\n",
    "            'text': result['text'],\n",
    "            'sections': json.dumps(result['sections']),\n",
    "            'tables': json.dumps(result['tables']),\n",
    "            'metadata': json.dumps(result['metadata']),\n",
    "            'parse_quality_score': quality_score,\n",
    "            'parse_errors': None\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'text': None,\n",
    "            'sections': None,\n",
    "            'tables': None,\n",
    "            'metadata': None,\n",
    "            'parse_quality_score': 0.0,\n",
    "            'parse_errors': str(e)\n",
    "        }\n",
    "\n",
    "def calculate_parse_quality(parsed_data):\n",
    "    \"\"\"Calculate quality score based on completeness\"\"\"\n",
    "    score = 0.0\n",
    "\n",
    "    # Has text\n",
    "    if parsed_data['text'] and len(parsed_data['text']) > 100:\n",
    "        score += 0.4\n",
    "\n",
    "    # Has sections\n",
    "    if len(parsed_data['sections']) > 0:\n",
    "        score += 0.3\n",
    "\n",
    "    # Has tables\n",
    "    if len(parsed_data['tables']) > 0:\n",
    "        score += 0.2\n",
    "\n",
    "    # Has metadata\n",
    "    if parsed_data['metadata']['num_pages'] > 0:\n",
    "        score += 0.1\n",
    "\n",
    "    return score\n",
    "\n",
    "# AutoLoader streaming ingestion\n",
    "raw_documents = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"binaryFile\")\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/Volumes/healthcare_catalog/raw_zone/checkpoints/schema\")\n",
    "    .option(\"cloudFiles.maxFilesPerTrigger\", 5)\n",
    "    .option(\"pathGlobFilter\", \"*.pdf\")\n",
    "    .load(\"/Volumes/healthcare_catalog/raw_zone/encounter_pdfs/\")\n",
    ")\n",
    "\n",
    "# Parse documents in streaming fashion\n",
    "parsed_documents = (raw_documents\n",
    "    .withColumn(\"document_id\", F.expr(\"uuid()\"))\n",
    "    .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"parsed_data\", parse_clinical_pdf(F.col(\"content\")))\n",
    "    .select(\n",
    "        \"document_id\",\n",
    "        \"path\",\n",
    "        \"ingestion_timestamp\",\n",
    "        \"parsed_data.*\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Write to Delta Lake (Bronze layer)\n",
    "(parsed_documents.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"/Volumes/healthcare_catalog/raw_zone/checkpoints/parsed_docs\")\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(\"healthcare_catalog.raw_zone.parsed_documents\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbecb92a-c531-4650-9d28-cc7dfb0a6d68",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 4"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM healthcare_catalog.raw_zone.parsed_documents"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6820793931751259,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_document_parsing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
